import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import warnings

# ==========================================
# 0. å…¨å±€è®¾ç½® (Global Settings)
# ==========================================
warnings.filterwarnings("ignore") # å¿½ç•¥ç‰ˆæœ¬å…¼å®¹æ€§è­¦å‘Š
np.random.seed(2026) # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æžœå¯å¤çŽ°

# è®¾ç½®å­¦æœ¯ç»˜å›¾é£Žæ ¼
sns.set_theme(style="whitegrid", context="paper", font_scale=1.2)

# ==========================================
# 1. æ¨¡åž‹å‚æ•° (æœ€ç»ˆæ ¡å‡†ç‰ˆ Final Calibration)
# ==========================================
# ä¿®æ­£é€»è¾‘ï¼š
# æå‡ BETA (ä¸‹è°ƒåŽ‹åŠ›) ä»¥ç¡®ä¿æ— æ•ˆè®°å¿†è¢«é—å¿˜ã€‚
# æå‡é‡æ”¾é—¨æ§› (Threshold) ä»¥å»ºç«‹ä¸¥æ ¼çš„ç­›é€‰æœºåˆ¶ã€‚

PARAMS = {
    'ALPHA': 0.22,  # â¬‡ï¸ é™ä½Žé‡æ”¾å¢žç›Š (é˜²æ­¢å™ªéŸ³è‡ªæˆ‘æ”¾å¤§)
    'BETA':  0.18,  # â¬†ï¸ æå‡ç¨³æ€ä¸‹è°ƒåŽ‹åŠ› (å…³é”®ï¼è¿™æ˜¯é—å¿˜çš„åŠ¨åŠ›)
    'GAMMA': 0.70,  # â¬†ï¸ æå‡è§‰é†’ç¡®è®¤å¢žç›Š (ä¿æŠ¤æœ‰æ•ˆè®°å¿†)
    'DELTA': 0.18,  # â¬†ï¸ æå‡æœ¬èƒ½æƒé‡ (æŠµæŠ—å¼ºä¸‹è°ƒåŽ‹åŠ›)
    'NOISE': 0.08   # ç”Ÿç‰©å™ªéŸ³
}

# æ¨¡æ‹Ÿè®¾ç½®
N_SIMS = 10000 
N_DAYS = 7     

# å®šä¹‰é¢œè‰²æ˜ å°„ (ä¿æŒè§†è§‰ä¸€è‡´æ€§)
PALETTE = {
    'A: Consolidation': '#1f77b4',  # è“ (å·©å›º)
    'B: Forgetting': '#2ca02c',     # ç»¿ (é—å¿˜ - æ³¨æ„é¢œè‰²å¯¹åº”)
    'C: Instinct': '#ff7f0e'        # æ©™ (æœ¬èƒ½)
}

def onph_step(w_prev, day, scenario):
    # --- é˜¶æ®µ 1: è§‰é†’æœŸ (Wake Phase) ---
    if scenario == 'Consolidation':
        # å­¦ä¹  + æŒç»­å¤ä¹ 
        input_signal = 0.9 if day == 1 else 0.6 * np.exp(-0.2 * day)
    elif scenario == 'Forgetting':
        # ä»… Day 1 æŽ¥è§¦ï¼Œä¹‹åŽæ— ç¡®è®¤
        input_signal = 0.9 if day == 1 else 0.0
    elif scenario == 'Instinct':
        input_signal = 0.0
    
    # å¼•å…¥è§‰é†’æœŸéšæœºæ€§
    wake_input = np.maximum(np.random.normal(input_signal, 0.1, N_SIMS), 0)
    w_wake = w_prev + PARAMS['GAMMA'] * wake_input
    
    # --- é˜¶æ®µ 2: ç¡çœ æœŸ (Sleep Phase) ---
    instinct_g = 1.0 if scenario == 'Instinct' else 0.0
    
    if scenario == 'Instinct':
        # æœ¬èƒ½å›žè·¯ï¼šç”±å†…æºæ€§é©±åŠ¨ï¼Œä¸ä¾èµ–å½“å‰æƒé‡
        replay = np.random.normal(0.8, 0.1, N_SIMS)
    else:
        # ã€å…³é”®ä¿®æ­£ã€‘Sigmoid é—¨æŽ§é‡æ”¾
        # å°†ä¸­å¿ƒç‚¹(é˜ˆå€¼)æå‡åˆ° 0.65ã€‚
        # åªæœ‰æƒé‡ > 0.65 çš„è®°å¿†ï¼Œæ‰æœ‰å¤§æ¦‚çŽ‡è§¦å‘å¼ºé‡æ”¾ã€‚
        # é—å¿˜ç»„(çº¦0.6)è¿‡ä¸åŽ»è¿™ä¸ªåŽï¼Œæ‰€ä»¥ä¼šè¢« BETA ä¿®å‰ªã€‚
        replay = (1 / (1 + np.exp(-12 * (w_wake - 0.65)))) * np.random.normal(0.9, 0.2, N_SIMS)

    downscaling = np.random.normal(1.0, 0.05, N_SIMS)
    
    # æ ¸å¿ƒåŠ¨åŠ›å­¦æ–¹ç¨‹
    delta = (PARAMS['ALPHA'] * replay) - (PARAMS['BETA'] * downscaling) + (PARAMS['DELTA'] * instinct_g)
    
    return np.clip(w_wake + delta + np.random.normal(0, PARAMS['NOISE'], N_SIMS), 0, 1)

def run_sim(scenario):
    data = np.zeros((N_SIMS, N_DAYS + 1))
    # åˆå§‹å€¼
    data[:, 0] = np.random.normal(0.6 if scenario == 'Instinct' else 0.0, 0.05, N_SIMS)
    for d in range(1, N_DAYS + 1): 
        data[:, d] = onph_step(data[:, d-1], d, scenario)
    return data

# ==========================================
# 2. ç”Ÿæˆå…¨é‡æ•°æ® (Big Data Generation)
# ==========================================
print("ðŸš€ æ­£åœ¨ç”Ÿæˆ 30,000 æ¡ä»¿çœŸè½¨è¿¹ (å·²åº”ç”¨å‚æ•°ä¿®æ­£)...")
raw_A = run_sim('Consolidation')
raw_B = run_sim('Forgetting')   # çŽ°åœ¨è¿™ç»„æ•°æ®åº”è¯¥ä¼šä¸‹é™
raw_C = run_sim('Instinct')

# æž„å»º DataFrame
def make_df(data, name):
    df = pd.DataFrame(data, columns=[f'Day_{i}' for i in range(N_DAYS+1)])
    df['Group'] = name
    return df

df_full = pd.concat([make_df(raw_A, 'A: Consolidation'), 
                     make_df(raw_B, 'B: Forgetting'), 
                     make_df(raw_C, 'C: Instinct')], ignore_index=True)

# å¯¼å‡ºåŽŸå§‹æ•°æ®
csv_filename = 'ONPH_Raw_Data_Full.csv'
df_full.to_csv(csv_filename, index=False)
print(f"âœ… åŽŸå§‹æ•°æ®å·²å¯¼å‡º: {csv_filename}")

# ==========================================
# 3. ANOVA æ–¹å·®åˆ†æž (Statistical Analysis)
# ==========================================
print("\nðŸ”¬ æ­£åœ¨è¿›è¡Œ ANOVA æ–¹å·®åˆ†æž (Day 7)...")

anova_data = df_full[['Group', 'Day_7']].rename(columns={'Day_7': 'Weight'})

# One-Way ANOVA
model = ols('Weight ~ C(Group)', data=anova_data).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

print("\n--- ANOVA Result ---")
print(anova_table)

# Tukey HSD
tukey = pairwise_tukeyhsd(endog=anova_data['Weight'], groups=anova_data['Group'], alpha=0.05)

# ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
with open('ONPH_Statistical_Report.txt', 'w', encoding='utf-8') as f:
    f.write("ONPH Hypothesis - Statistical Analysis Report\n")
    f.write("============================================\n")
    f.write(f"Parameters: ALPHA={PARAMS['ALPHA']}, BETA={PARAMS['BETA']}, Threshold=0.65\n\n")
    f.write("1. ANOVA Table\n")
    f.write(anova_table.to_string())
    f.write("\n\n2. Tukey HSD Results\n")
    f.write(str(tukey))

print("âœ… ç»Ÿè®¡æŠ¥å‘Šå·²å¯¼å‡º: ONPH_Statistical_Report.txt")

# ==========================================
# 4. å¤šç»´å¯è§†åŒ– (Visualization)
# ==========================================
print("\nðŸŽ¨ æ­£åœ¨ç»˜åˆ¶æœ€ç»ˆå›¾è¡¨...")

fig = plt.figure(figsize=(18, 12), dpi=200)
gs = fig.add_gridspec(2, 2)

# --- å›¾ A: æ—¶é—´æ¼”åŒ–è¶‹åŠ¿å›¾ ---
ax1 = fig.add_subplot(gs[0, :])
plot_df = df_full.sample(2000, random_state=42).melt(id_vars=['Group'], var_name='Day', value_name='Weight')
plot_df['Day'] = plot_df['Day'].str.extract(r'(\d+)').astype(int)

# ç»˜åˆ¶æŠ˜çº¿å›¾
sns.lineplot(data=plot_df, x='Day', y='Weight', hue='Group', style='Group', 
             palette=PALETTE, linewidth=3, ax=ax1)

ax1.set_title('(A) Temporal Evolution: The "Bifurcation" of Memory Fate', fontsize=16, fontweight='bold')
ax1.set_ylim(-0.05, 1.05)
ax1.set_ylabel('Synaptic Weight ($W_{ij}$)')
# æ ‡æ³¨é—¨æ§›çº¿
ax1.axhline(0.65, ls='--', color='red', alpha=0.5, label='Replay Threshold (0.65)')
ax1.text(0.1, 0.67, 'Replay Threshold (Selection Filter)', color='red', fontsize=10)
ax1.legend(loc='lower right', frameon=True, framealpha=0.9)

# --- å›¾ B: æœ€ç»ˆåˆ†å¸ƒç›´æ–¹å›¾ ---
ax2 = fig.add_subplot(gs[1, 0])
sns.histplot(data=anova_data, x='Weight', hue='Group', element="step", stat="density", common_norm=False, 
             palette=PALETTE, alpha=0.3, ax=ax2)
sns.kdeplot(data=anova_data, x='Weight', hue='Group', fill=False, linewidth=2.5, common_norm=False, 
            palette=PALETTE, legend=False, ax=ax2)
ax2.set_title('(B) Distribution of Final Weights (Day 7)', fontsize=16, fontweight='bold')

# --- å›¾ C: å°æç´å›¾ ---
ax3 = fig.add_subplot(gs[1, 1])
sns.violinplot(data=anova_data, x='Group', y='Weight', hue='Group', legend=False, inner=None, 
               palette=PALETTE, alpha=0.4, ax=ax3)

sample_scatter = anova_data.groupby('Group').sample(300)
sns.stripplot(data=sample_scatter, x='Group', y='Weight', hue='Group', legend=False,
              color='black', size=1.5, alpha=0.3, ax=ax3)

ax3.set_title('(C) Statistical Variance Analysis', fontsize=16, fontweight='bold')
ax3.set_xticks(range(3))
ax3.set_xticklabels(['Consolidation', 'Forgetting', 'Instinct'])

# æ ‡æ³¨ P å€¼
p_val = anova_table["PR(>F)"].iloc[0]
ax3.text(0.5, 1.05, f'ANOVA p < {p_val:.1e}', ha='center', color='darkred', fontweight='bold', transform=ax3.transAxes)

plt.tight_layout()
plt.savefig('ONPH_Final_Validated_Plot.png')
plt.show()

print("âœ… æœ€ç»ˆå›¾è¡¨å·²å¯¼å‡º: ONPH_Final_Validated_Plot.png")
print("ðŸŽ‰ éªŒè¯å®Œæˆï¼é€»è¾‘æ¼æ´žå·²ä¿®å¤ï¼šé—å¿˜ç»„çŽ°åœ¨æ­£ç¡®åœ°è¡¨çŽ°ä¸ºæƒé‡è¡°å‡ã€‚")
